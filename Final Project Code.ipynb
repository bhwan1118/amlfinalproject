{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML FINAL PROJECT ##\n",
    "by: Zeheng Wang, Benjamin Hwang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word2Vec models:##\n",
    "\n",
    "#### a) Preprocsesing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import word2vec and libraries needed. Define the training, dev, and test numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_train = 8000\n",
    "num_dev = 2000\n",
    "num_test = 2000\n",
    "split_idx = list(range(num_train + num_dev))\n",
    "random.shuffle(split_idx)\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions to help process the descriptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add additional preprocessing to get rid of stop words from descriptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word preprocessing libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import string\n",
    "\n",
    "# Downloads if necessary:\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def preprocess(docs):\n",
    "    docs = list(map(lambda a: a.lower(), docs))\n",
    "    docs = list(map(lambda a: a.translate(str.maketrans('','', string.punctuation)), docs))\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    docs = list(map(lambda x: ' '.join([word for word in x.split() if word not in (stop)]), docs))\n",
    "\n",
    "    #Lemmatization of all the words\n",
    "    lem = WordNetLemmatizer()\n",
    "    docs = list(map(lambda x: ' '.join([lem.lemmatize(word) for word in x.split()]), docs))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the initial description features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build x matrices\n",
    "\n",
    "train_dev_desc = preprocess(parse_descriptions(\"descriptions_train\", num_doc=(num_train+num_dev)))\n",
    "test_desc = preprocess(parse_descriptions(\"descriptions_test\", num_doc=num_test))\n",
    "x_train_desc = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[:num_train]])\n",
    "x_dev_desc = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[num_train:]])\n",
    "x_test_desc = np.array([doc_to_vec(d, word2vec) for d in test_desc])\n",
    "\n",
    "print(\"Built all x matrices!\")\n",
    "print(\"x_train shape:\", x_train_desc.shape)\n",
    "print(\"x_dev shape:\", x_dev_desc.shape)\n",
    "print(\"x_test shape:\", x_test_desc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function to parse the ResNet features to be predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use SVD to compress intermediate ResNet features for an optimal representation of ResNet for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to generate the matrix needed given a specific rank\n",
    "def FMatrix_Generator(Matrix, vh, r):\n",
    "    F_Mat = np.matmul(Matrix, np.transpose(vh[:r,:]))\n",
    "    \n",
    "    return F_Mat\n",
    "\n",
    "y_original = parse_features(\"features_train/features_resnet1000intermediate_train.csv\")\n",
    "u, s, vh = np.linalg.svd(y_original, full_matrices = True)\n",
    "y_train_dev = FMatrix_Generator(y_original, vh, 50)\n",
    "y_train = y_train_dev[split_idx[:num_train]]\n",
    "\n",
    "y_dev = y_train_dev[split_idx[num_train:]]\n",
    "\n",
    "y_test_original = parse_features(\"features_test/features_resnet1000intermediate_test.csv\")\n",
    "y_test = FMatrix_Generator(y_test_original, vh, 50)\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_dev shape:\", y_dev.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train OLS model with regression\n",
    "parameters = {\"alpha\": [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    "reg = GridSearchCV(Ridge(), parameters, cv = 10)\n",
    "reg.fit(x_train, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN A RANDOM FOREST REGRESSOR TO TEST:\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#parameters = {\"n_estimators\": [5, 10, 20, 50, 100], \"max_depth\": [2, 5, 10, 20, 50]}\n",
    "#rf_reg = GridSearchCV(RandomForestRegressor(), parameters, cv = 10)\n",
    "#rf_reg.fit(x_train, y_train)\n",
    "#rf_reg_best = rf_reg.best_estimator_\n",
    "#print(rf_reg_best)\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators = 5, max_depth = 50)\n",
    "rf_reg.fit(x_train_conc, y_train)\n",
    "rf_reg_best = rf_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Ridge Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Kernel Ridge Regression\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "parameters = {\"alpha\": [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0], \"kernel\": ['linear', 'rbf', 'poly']}\n",
    "reg = GridSearchCV(KernelRidge(), parameters, cv = 10)\n",
    "reg.fit(x_train_conc, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now score the models and assess performance on dev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cdist_matrix(x1, x2):\n",
    "    return distance.cdist(x1, x2, 'cosine')\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(x_dev)\n",
    "dev_distances = cdist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(num_dev):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train, x_dev])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "reg_best.fit(x_train_all, y_train_all)\n",
    "\n",
    "# testing to make sure this works\n",
    "#reg.fit(x_train_all, y_train_all)\n",
    "#y_test_pred = reg.predict(x_test)\n",
    "\n",
    "y_test_pred = reg_best.predict(x_test)\n",
    "test_distances = cdist_matrix(y_test_pred, y_test)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words models:##\n",
    "\n",
    "**a) Preprocessing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Represent the Training Data as a bag of words:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Bag of words instead of word2vec for training data\n",
    "\n",
    "train_dev_desc = preprocess(parse_descriptions(\"descriptions_train\", num_doc=(num_train + num_dev)))\n",
    "test_desc = preprocess(parse_descriptions(\"descriptions_test\", num_doc = num_test))\n",
    "\n",
    "bag_train_list = []\n",
    "bag_array = []\n",
    "for i in train_dev_desc:\n",
    "    bag_array.append(i.split())\n",
    "    \n",
    "for i in bag_array:\n",
    "    for j in i:\n",
    "        bag_train_list.append(j)\n",
    "        \n",
    "train_dictionary = list(set(bag_train_list))\n",
    "\n",
    "train_dict = []\n",
    "for i in bag_array:\n",
    "    unique, counts = np.unique(i, return_counts = True)\n",
    "    train_dict.append(dict(zip(unique,counts)))\n",
    "    \n",
    "# pandas data frame to store the unique list of words per training set:\n",
    "import pandas as pd\n",
    "\n",
    "train_bag_of_words = pd.DataFrame(0, index=np.arange(len(train_dict)), columns = train_dictionary)\n",
    "\n",
    "# write the data into each \n",
    "for cols in train_bag_of_words:\n",
    "    for i in range(len(train_dict)):\n",
    "        if cols in train_dict[i].keys():\n",
    "            train_bag_of_words[cols][i] = train_dict[i][cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now Convert the Test Data into a bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Bag of words instead of word2vec for testing\n",
    "\n",
    "bag_test_list = []\n",
    "bag_test_array = []\n",
    "for i in test_desc:\n",
    "    bag_test_array.append(i.split())\n",
    "    \n",
    "for i in bag_test_array:\n",
    "    for j in i:\n",
    "        bag_test_list.append(j)\n",
    "        \n",
    "\n",
    "test_dict = []\n",
    "for i in bag_test_array:\n",
    "    unique, counts = np.unique(i, return_counts = True)\n",
    "    test_dict.append(dict(zip(unique,counts)))\n",
    "    \n",
    "# pandas data frame to store the unique list of words per training set:\n",
    "import pandas as pd\n",
    "\n",
    "test_bag_of_words = pd.DataFrame(0, index=np.arange(len(test_dict)), columns = train_dictionary)\n",
    "\n",
    "# write the data into each \n",
    "for cols in test_bag_of_words:\n",
    "    for i in range(len(test_dict)):\n",
    "        if cols in test_dict[i].keys():\n",
    "            test_bag_of_words[cols][i] = test_dict[i][cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now Create the Training and Test X Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_bag = np.array(train_bag_of_words)\n",
    "test_bag = np.array(test_bag_of_words)\n",
    "\n",
    "x_train = train_dev_bag[split_idx[:num_train]]\n",
    "x_dev = train_dev_bag[split_idx[num_train:]]\n",
    "x_test = test_bag\n",
    "\n",
    "print(\"Built all x matrices!\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_dev shape:\", x_dev.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to generate the matrix needed given a specific rank\n",
    "def FMatrix_Generator(Matrix, vh, r):\n",
    "    F_Mat = np.matmul(Matrix, np.transpose(vh[:r,:]))\n",
    "    \n",
    "    return F_Mat\n",
    "\n",
    "y_original = parse_features(\"features_train/features_resnet1000intermediate_train.csv\")\n",
    "u, s, vh = np.linalg.svd(y_original, full_matrices = True)\n",
    "y_train_dev = FMatrix_Generator(y_original, vh, 50)\n",
    "y_train = y_train_dev[split_idx[:num_train]]\n",
    "\n",
    "y_dev = y_train_dev[split_idx[num_train:]]\n",
    "\n",
    "y_test_original = parse_features(\"features_test/features_resnet1000intermediate_test.csv\")\n",
    "y_test = FMatrix_Generator(y_test_original, vh, 50)\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_dev shape:\", y_dev.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"alpha\": [10]}\n",
    "reg = GridSearchCV(Ridge(), parameters, cv=10)\n",
    "# reg = MultiTaskElasticNetCV(cv=5, random_state=0)\n",
    "\n",
    "\n",
    "reg.fit(x_train, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now score the models and assess performance on dev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cdist_matrix(x1, x2):\n",
    "    return distance.cdist(x1, x2, 'cosine')\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(x_dev)\n",
    "dev_distances = cdist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(num_dev):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train, x_dev])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "reg_best.fit(x_train_all, y_train_all)\n",
    "\n",
    "# testing to make sure this works\n",
    "#reg.fit(x_train_all, y_train_all)\n",
    "#y_test_pred = reg.predict(x_test)\n",
    "\n",
    "y_test_pred = reg_best.predict(x_test)\n",
    "test_distances = cdist_matrix(y_test_pred, y_test)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
