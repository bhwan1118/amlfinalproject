{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods Explored: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhwan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for Experiementing with noun extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Real Preprocessing\n",
    "\n",
    "#Word preprocessing\n",
    "import nltk\n",
    "\n",
    "# Downloads if necessary:\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "##nltk.download('punkt')\n",
    "##nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import string\n",
    "\n",
    "def preprocess(docs):\n",
    "    docs = list(map(lambda a: a.lower(), docs))\n",
    "    docs = list(map(lambda a: a.translate(str.maketrans('','', string.punctuation)), docs))\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    docs = list(map(lambda x: ' '.join([word for word in x.split() if word not in (stop)]), docs))\n",
    "\n",
    "    #Lemmatization of all the words\n",
    "    lem = WordNetLemmatizer()\n",
    "    docs = list(map(lambda x: ' '.join([lem.lemmatize(word) for word in x.split()]), docs))\n",
    "    # docs = list(set(docs))\n",
    "\n",
    "    # nouns only ******* this is added\n",
    "    # docs = list(map(lambda x: ' '.join([word for (word, pos) in nltk.pos_tag(nltk.word_tokenize(x)) if pos[0] == 'N']),docs))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this without the noun extraction first\n",
    "train_dev_desc = preprocess(parse_descriptions(\"descriptions_train\", num_doc=(10000)))\n",
    "test_desc = preprocess(parse_descriptions(\"descriptions_test\", num_doc=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for exploring Tfidf (term frequencyâ€“inverse document frequency) Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_vectorizer = TfidfVectorizer()\n",
    "train_vectorizer.fit(train_dev_desc)\n",
    "X_train = train_vectorizer.transform(train_dev_desc)\n",
    "X_test = train_vectorizer.transform(test_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions From Original Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all y matrices!\n"
     ]
    }
   ],
   "source": [
    "# define a helper function to generate the matrix needed given a specific rank\n",
    "def FMatrix_Generator(Matrix, vh, r):\n",
    "    F_Mat = np.matmul(Matrix, np.transpose(vh[:r,:]))\n",
    "    \n",
    "    return F_Mat\n",
    "\n",
    "y_original = parse_features(\"features_train/features_resnet1000intermediate_train.csv\")\n",
    "u, s, vh = np.linalg.svd(y_original, full_matrices = True)\n",
    "y_train_dev = FMatrix_Generator(y_original, vh, 50)\n",
    "##y_train = y_train_dev[split_idx[:num_train]]\n",
    "\n",
    "##y_dev = y_train_dev[split_idx[num_train:]]\n",
    "\n",
    "y_test_original = parse_features(\"features_test/features_resnet1000intermediate_test.csv\")\n",
    "y_test = FMatrix_Generator(y_test_original, vh, 50)\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "#print(\"y_train shape:\", y_train_dev.shape)\n",
    "#print(\"y_dev shape:\", y_dev.shape)\n",
    "#print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "    X_train, y_train_dev, test_size = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Trained and Tested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained linear regression model!\n",
      "Summary of best model:\n",
      "Ridge(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train OLS model with regression\n",
    "parameters = {\"alpha\": [0, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]} #ALPHA 5 best\n",
    "\n",
    "#parameters = {\"alpha\": [5]}\n",
    "reg = GridSearchCV(Ridge(), parameters, cv=10)\n",
    "# reg = MultiTaskElasticNetCV(cv=5, random_state=0)\n",
    "\n",
    "reg.fit(x_train, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained linear regression model!\n",
      "Summary of best model:\n",
      "Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "parameters = {\"alpha\": [0.5]} #ALPHA\n",
    "reg = GridSearchCV(Lasso(), parameters, cv=10)\n",
    "\n",
    "reg.fit(x_train, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "parameters = {\"max_depth\": [10], \"n_estimators\": [10]}\n",
    "\n",
    "reg = GridSearchCV(RandomForestRegressor(), parameters, cv = 10)\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "#reg = RandomForestRegressor(max_depth = 1000, n_estimators = 3).fit(x_train, y_train)\n",
    "reg_best = reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.18774034731608258\n",
      "Mean index of true image 46.252\n",
      "Median index of true image 14.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cdist_matrix(x1, x2):\n",
    "    return distance.cdist(x1, x2, 'cosine')\n",
    "\n",
    "def dist_matrix(x1, x2):\n",
    "    return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(x_dev)\n",
    "dev_distances = cdist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(2000):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "# create test predictions\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "x_train_all = vstack([x_train, x_dev])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "reg_best.fit(x_train_all, y_train_all)\n",
    "y_test_pred = reg_best.predict(X_test)\n",
    "test_distances = cdist_matrix(y_test_pred, y_test)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(2000):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-Grams Experimentation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Last thing to test... Using N-Grams:\n",
    "\n",
    "N_gram_train_desc = preprocess(parse_descriptions(\"descriptions_train\", num_doc=(10000)))\n",
    "N_gram_test_desc = preprocess(parse_descriptions(\"descriptions_test\", num_doc=(2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "n = 2\n",
    "\n",
    "bag_train_list = []\n",
    "bag_array = []\n",
    "for i in N_gram_train_desc:\n",
    "    bag_array.append(list(ngrams(i.split(),n)))\n",
    "    \n",
    "for i in bag_array:\n",
    "    for j in i:\n",
    "        bag_train_list.append(j)\n",
    "        \n",
    "train_dictionary = list(set(bag_train_list))\n",
    "\n",
    "train_dict = []\n",
    "for i in bag_array:\n",
    "    unique, counts = np.unique(i, return_counts = True)\n",
    "    train_dict.append(dict(zip(unique,counts)))\n",
    "    \n",
    "# pandas data frame to store the unique list of words per training set:\n",
    "import pandas as pd\n",
    "\n",
    "train_bag_of_words = pd.DataFrame(0, index=np.arange(len(train_dict)), columns = train_dictionary)\n",
    "\n",
    "# write the data into each \n",
    "for cols in train_bag_of_words:\n",
    "    for i in range(len(train_dict)):\n",
    "        if cols in train_dict[i].keys():\n",
    "            train_bag_of_words[cols][i] = train_dict[i][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 107193)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams Bag of Words\n",
    "\n",
    "bag_test_list = []\n",
    "bag_test_array = []\n",
    "for i in N_gram_test_desc:\n",
    "    bag_test_array.append(list(ngrams(i.split(),n)))\n",
    "    \n",
    "for i in bag_test_array:\n",
    "    for j in i:\n",
    "        bag_test_list.append(j)\n",
    "        \n",
    "\n",
    "test_dict = []\n",
    "for i in bag_test_array:\n",
    "    unique, counts = np.unique(i, return_counts = True)\n",
    "    test_dict.append(dict(zip(unique,counts)))\n",
    "    \n",
    "# pandas data frame to store the unique list of words per training set:\n",
    "import pandas as pd\n",
    "\n",
    "test_bag_of_words = pd.DataFrame(0, index=np.arange(len(test_dict)), columns = train_dictionary)\n",
    "\n",
    "# write the data into each \n",
    "for cols in test_bag_of_words:\n",
    "    for i in range(len(test_dict)):\n",
    "        if cols in test_dict[i].keys():\n",
    "            test_bag_of_words[cols][i] = test_dict[i][cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_bag = np.array(train_bag_of_words)\n",
    "\n",
    "x_test = np.array(test_bag_of_words)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "    train_dev_bag, y_train_dev, test_size = 2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Tested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained linear regression model!\n",
      "Summary of best model:\n",
      "Ridge(alpha=50, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train OLS model with regression\n",
    "parameters = {\"alpha\": [50]} #ALPHA 5 best\n",
    "\n",
    "#parameters = {\"alpha\": [5]}\n",
    "reg = GridSearchCV(Ridge(), parameters, cv=10)\n",
    "# reg = MultiTaskElasticNetCV(cv=5, random_state=0)\n",
    "\n",
    "reg.fit(x_train, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.0017988698285718407\n",
      "Mean index of true image 999.5\n",
      "Median index of true image 999.5\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cdist_matrix(x1, x2):\n",
    "    return distance.cdist(x1, x2, 'cosine')\n",
    "\n",
    "def dist_matrix(x1, x2):\n",
    "    return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(x_dev)\n",
    "dev_distances = cdist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(2000):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA experimentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA ATTEMPT\n",
    "\n",
    "## Create Bag of words instead of word2vec for training data\n",
    "\n",
    "train_dev_desc = preprocess(parse_descriptions(\"descriptions_train\", num_doc=(10000)))\n",
    "test_desc = preprocess(parse_descriptions(\"descriptions_test\", num_doc=2000))\n",
    "\n",
    "bag_train_list = []\n",
    "bag_array = []\n",
    "for i in train_dev_desc:\n",
    "    bag_array.append(i.split())\n",
    "    \n",
    "for i in bag_array:\n",
    "    for j in i:\n",
    "        bag_train_list.append(j)\n",
    "        \n",
    "train_dictionary = list(set(bag_train_list))\n",
    "\n",
    "train_dict = []\n",
    "for i in bag_array:\n",
    "    unique, counts = np.unique(i, return_counts = True)\n",
    "    train_dict.append(dict(zip(unique,counts)))\n",
    "    \n",
    "# pandas data frame to store the unique list of words per training set:\n",
    "import pandas as pd\n",
    "\n",
    "train_bag_of_words = pd.DataFrame(0, index=np.arange(len(train_dict)), columns = train_dictionary)\n",
    "\n",
    "# write the data into each \n",
    "for cols in train_bag_of_words:\n",
    "    for i in range(len(train_dict)):\n",
    "        if cols in train_dict[i].keys():\n",
    "            train_bag_of_words[cols][i] = train_dict[i][cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Bag of words instead of word2vec for testing\n",
    "\n",
    "bag_test_list = []\n",
    "bag_test_array = []\n",
    "for i in test_desc:\n",
    "    bag_test_array.append(i.split())\n",
    "    \n",
    "for i in bag_test_array:\n",
    "    for j in i:\n",
    "        bag_test_list.append(j)\n",
    "        \n",
    "\n",
    "test_dict = []\n",
    "for i in bag_test_array:\n",
    "    unique, counts = np.unique(i, return_counts = True)\n",
    "    test_dict.append(dict(zip(unique,counts)))\n",
    "    \n",
    "# pandas data frame to store the unique list of words per training set:\n",
    "import pandas as pd\n",
    "\n",
    "test_bag_of_words = pd.DataFrame(0, index=np.arange(len(test_dict)), columns = train_dictionary)\n",
    "\n",
    "# write the data into each \n",
    "for cols in test_bag_of_words:\n",
    "    for i in range(len(test_dict)):\n",
    "        if cols in test_dict[i].keys():\n",
    "            test_bag_of_words[cols][i] = test_dict[i][cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8338)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_dev_bag = np.array(train_bag_of_words)\n",
    "x_test = np.array(test_bag_of_words)\n",
    "\n",
    "X = np.concatenate((train_dev_bag, x_test))\n",
    "\n",
    "pca = PCA(n_components = 4000)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dev = X_pca[:10000]\n",
    "x_test = X_pca[-2000:]\n",
    "\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "    x_train_dev, y_train_dev, test_size = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained linear regression model!\n",
      "Summary of best model:\n",
      "Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train OLS model with regression\n",
    "#parameters = {\"alpha\": [20, 200, 300, 1000]} #ALPHA 5 best\n",
    "\n",
    "parameters = {\"alpha\": [50, 75, 100, 125]}\n",
    "reg = GridSearchCV(Ridge(), parameters, cv=10)\n",
    "# reg = MultiTaskElasticNetCV(cv=5, random_state=0)\n",
    "\n",
    "reg.fit(x_train, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.28030375196054375\n",
      "Mean index of true image 25.142\n",
      "Median index of true image 7.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cdist_matrix(x1, x2):\n",
    "    return distance.cdist(x1, x2, 'cosine')\n",
    "\n",
    "def dist_matrix(x1, x2):\n",
    "    return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(x_dev)\n",
    "dev_distances = cdist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(2000):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train, x_dev])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "reg_best.fit(x_train_all, y_train_all)\n",
    "y_test_pred = reg_best.predict(x_test)\n",
    "test_distances = cdist_matrix(y_test_pred, y_test)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(2000):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
